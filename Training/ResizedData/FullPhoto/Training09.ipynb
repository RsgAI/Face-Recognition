{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training9\n",
    "\n",
    "In this notebook file, ResizedData-FullPhoto dataset will be read from pkl file.\n",
    "\n",
    "Input(X) and Output(Y) numpy arrays will be created from pandas dataframes.\n",
    "\n",
    "VGG16 pre-trained model will be load and used.\n",
    "\n",
    "Pre-trained model's layers except the last CNN block will be set to non-trainable.\n",
    "\n",
    "Training will be performed in the sections between the last CNN block and the Output layer.\n",
    "\n",
    "In this way, the experience gained by the model on very large datasets will be used in this classification problem, while the last CNN block will be updated and fine-tuned.\n",
    "\n",
    "This method is known as [**Fine-Tuning**](https://deeplizard.com/learn/video/5T-iXNNiwIs \"deeplizard\").\n",
    "\n",
    "See also [**Transfer Learning and Fine-Tuning**](https://www.tensorflow.org/tutorials/images/transfer_learning \"tensorflow\").\n",
    "\n",
    "A keras utils Sequence class will be defined so that operations can be performed on the data to be used during the training.\n",
    "\n",
    "Performance will be checked with Validation data while training model with Training data.\n",
    "\n",
    "Accuracy and Loss charts will be drawn according to epoch numbers.\n",
    "\n",
    "The results obtained by evaluating the model with Test data will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries are being imported\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.22.3\n",
      "pandas Version: 1.4.3\n",
      "tensorflow Version: 2.6.0\n",
      "matplotlib Version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "#Library versions are being printed\n",
    "print('numpy Version: ' + np.__version__)\n",
    "print('pandas Version: ' + pd.__version__)\n",
    "print('tensorflow Version: ' + tf.__version__)\n",
    "print('matplotlib Version: ' + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#GPU will be used for training\n",
    "myGPU = tf.test.gpu_device_name()\n",
    "if myGPU:\n",
    "    print(myGPU)\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abdullah Gul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrien Brody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmed Chalabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ai Sugiyama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Greenspan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Yasser Arafat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Yoko Ono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Yoriko Kawaguchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Zhu Rongji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Zinedine Zidane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name\n",
       "ID                   \n",
       "0        Abdullah Gul\n",
       "1        Adrien Brody\n",
       "2       Ahmed Chalabi\n",
       "3         Ai Sugiyama\n",
       "4      Alan Greenspan\n",
       "..                ...\n",
       "418     Yasser Arafat\n",
       "419          Yoko Ono\n",
       "420  Yoriko Kawaguchi\n",
       "421        Zhu Rongji\n",
       "422   Zinedine Zidane\n",
       "\n",
       "[423 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Person dataframe in ResizedData is being read from md5 file\n",
    "personDf = pd.read_pickle(\"../../../Data/ResizedData/Person.pkl\")\n",
    "personDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>[[[42, 19, 17], [42, 19, 17], [42, 19, 17], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>[[[29, 43, 55], [29, 43, 55], [28, 42, 54], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356</td>\n",
       "      <td>[[[59, 59, 53], [58, 58, 52], [57, 56, 51], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>277</td>\n",
       "      <td>[[[37, 32, 29], [37, 32, 31], [38, 33, 32], [3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>322</td>\n",
       "      <td>[[[193, 202, 211], [193, 202, 211], [193, 205,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>383</td>\n",
       "      <td>[[[10, 13, 51], [10, 13, 51], [11, 13, 53], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>44</td>\n",
       "      <td>[[[8, 14, 19], [8, 14, 19], [8, 14, 19], [9, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4156 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PersonID                                           ImageBGR\n",
       "0           22  [[[42, 19, 17], [42, 19, 17], [42, 19, 17], [4...\n",
       "1          125  [[[29, 43, 55], [29, 43, 55], [28, 42, 54], [2...\n",
       "2          356  [[[59, 59, 53], [58, 58, 52], [57, 56, 51], [5...\n",
       "3          277  [[[37, 32, 29], [37, 32, 31], [38, 33, 32], [3...\n",
       "4          131  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n",
       "...        ...                                                ...\n",
       "4151         4  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n",
       "4152       120  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n",
       "4153       322  [[[193, 202, 211], [193, 202, 211], [193, 205,...\n",
       "4154       383  [[[10, 13, 51], [10, 13, 51], [11, 13, 53], [1...\n",
       "4155        44  [[[8, 14, 19], [8, 14, 19], [8, 14, 19], [9, 1...\n",
       "\n",
       "[4156 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FullPhoto Training data is being read from md5 file\n",
    "trainingDf = pd.read_pickle(\"../../../Data/ResizedData/FullPhoto/Training.pkl\")\n",
    "trainingDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainingX is being extracted from trainingDf as wanted shape\n",
    "#trainingX contains images with pixel values of data type np.uint8 in the range [0, 255]\n",
    "\n",
    "#Many pre-trained models, including the ones to be used within the scope of this project,\n",
    "#have been trained with images containing pixel values in the [-1, 1] range\n",
    "#In this way, the data will be symmetrical and the performance of the Backpropagation algorithm will be increased\n",
    "#See https://en.wikipedia.org/wiki/Backpropagation\n",
    "#See also https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data\n",
    "#Therefore, training will be performed by converting pixel values to this range with the simplest method (pixel / 127.5 - 1)\n",
    "\n",
    "#Converting pixel values to range [-1, 1] in this section is an option\n",
    "#Doing this once over the entire array now will be save time\n",
    "#This is not how the conversion will be done because of some memory problems in this project\n",
    "#Images are of data type np.uint8 when they are in the range [0, 255]\n",
    "#np.uint8 requires 1 byte memory while np.float32 requires 4 byte and np.float64 requires 8 byte\n",
    "#See https://www.educba.com/numpy-data-types/\n",
    "#When np.uint8 data type, images use about 1GB memory\n",
    "#Even if these pixel values are converted to np.float32 data type, it will need about 4GB of memory\n",
    "#The computer used for this project has 8GB Ram\n",
    "#Considering operating system requirements, memory required by the model, etc. 8GB Ram is not enough for this process\n",
    "#For this reason, this method is not preferred, although it will save time\n",
    "\n",
    "trainingX = np.array(trainingDf.ImageBGR.values.tolist())\n",
    "trainingX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainingY is being extracted from trainingDf as wanted shape\n",
    "trainingY = np.array(trainingDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "trainingY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>[[[28, 8, 3], [28, 8, 3], [28, 8, 3], [28, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[13, 20, 15], [13, 20, 15], [13, 20, 15], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196</td>\n",
       "      <td>[[[60, 67, 60], [60, 67, 60], [60, 67, 60], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95</td>\n",
       "      <td>[[[94, 131, 175], [98, 135, 179], [99, 136, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>380</td>\n",
       "      <td>[[[84, 37, 15], [83, 36, 14], [83, 36, 14], [8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>71</td>\n",
       "      <td>[[[2, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>183</td>\n",
       "      <td>[[[58, 34, 22], [52, 28, 18], [44, 22, 11], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[4, 0, 0], [2, 0, 0], [2, 0, 0], [1, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[27, 51, 71], [26, 50, 70], [26, 50, 68], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>209</td>\n",
       "      <td>[[[0, 2, 0], [0, 2, 0], [0, 2, 0], [0, 3, 0], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>914 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PersonID                                           ImageBGR\n",
       "0         171  [[[28, 8, 3], [28, 8, 3], [28, 8, 3], [28, 8, ...\n",
       "1         120  [[[13, 20, 15], [13, 20, 15], [13, 20, 15], [1...\n",
       "2         196  [[[60, 67, 60], [60, 67, 60], [60, 67, 60], [5...\n",
       "3          95  [[[94, 131, 175], [98, 135, 179], [99, 136, 18...\n",
       "4         380  [[[84, 37, 15], [83, 36, 14], [83, 36, 14], [8...\n",
       "..        ...                                                ...\n",
       "909        71  [[[2, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], ...\n",
       "910       183  [[[58, 34, 22], [52, 28, 18], [44, 22, 11], [4...\n",
       "911       120  [[[4, 0, 0], [2, 0, 0], [2, 0, 0], [1, 0, 0], ...\n",
       "912       120  [[[27, 51, 71], [26, 50, 70], [26, 50, 68], [2...\n",
       "913       209  [[[0, 2, 0], [0, 2, 0], [0, 2, 0], [0, 3, 0], ...\n",
       "\n",
       "[914 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FullPhoto Validation data is being read from md5 file\n",
    "validationDf = pd.read_pickle(\"../../../Data/ResizedData/FullPhoto/Validation.pkl\")\n",
    "validationDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validationX is being extracted from validationDf as wanted shape\n",
    "#validationX contains images with pixel values of data type np.uint8 in the range [0, 255]\n",
    "\n",
    "#Many pre-trained models, including the ones to be used within the scope of this project,\n",
    "#have been trained with images containing pixel values in the [-1, 1] range\n",
    "#In this way, the data will be symmetrical and the performance of the Backpropagation algorithm will be increased\n",
    "#See https://en.wikipedia.org/wiki/Backpropagation\n",
    "#See also https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data\n",
    "#Therefore, training will be performed by converting pixel values to this range with the simplest method (pixel / 127.5 - 1)\n",
    "\n",
    "#Converting pixel values to range [-1, 1] in this section is an option\n",
    "#Doing this once over the entire array now will be save time\n",
    "#This is not how the conversion will be done because of some memory problems in this project\n",
    "#Images are of data type np.uint8 when they are in the range [0, 255]\n",
    "#np.uint8 requires 1 byte memory while np.float32 requires 4 byte and np.float64 requires 8 byte\n",
    "#See https://www.educba.com/numpy-data-types/\n",
    "#When np.uint8 data type, images use about 1GB memory\n",
    "#Even if these pixel values are converted to np.float32 data type, it will need about 4GB of memory\n",
    "#The computer used for this project has 8GB Ram\n",
    "#Considering operating system requirements, memory required by the model, etc. 8GB Ram is not enough for this process\n",
    "#For this reason, this method is not preferred, although it will save time\n",
    "\n",
    "validationX = np.array(validationDf.ImageBGR.values.tolist())\n",
    "validationX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validationY is being extracted from validationDf as wanted shape\n",
    "validationY = np.array(validationDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "validationY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VGG16 pre-trained model is being loaded\n",
    "#The original VGG16 model was trained with images with size of (224, 224, 3) \n",
    "#in BGR color order and pixel values of [-1, 1] (zero centered) as default\n",
    "#See https://keras.io/api/applications/vgg/ for more information\n",
    "#Since images of dataset saved as size of (224, 224, 3) in BGR color order and pixel values of [0, 255]\n",
    "#dataset will be used by just converting the pixel values to the range [-1, 1]\n",
    "\n",
    "#Training will be performed in the sections between the last CNN block and the Output layer\n",
    "\n",
    "model = tf.keras.applications.vgg16.VGG16(include_top = False, input_shape = ((224, 224, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of layers of pre-trained model is being calculated except last CNN block\n",
    "nonTrainablePart = len(model.layers) - 4\n",
    "nonTrainablePart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Pre-trained model's layers except the last CNN block is being set to non-trainable\n",
    "for layer in model.layers[:nonTrainablePart]:\n",
    "    layer.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 423)               866727    \n",
      "=================================================================\n",
      "Total params: 71,160,039\n",
      "Trainable params: 63,524,775\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#The pre-trained model is being connected to the fully connected layer\n",
    "#A dropout layer is being added to the the model to prevent overfitting,\n",
    "#and the model is being completed with the addition of the output layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(2048, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2048, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(personDf.shape[0], activation = tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model is being compiled with Adam optimizer\n",
    "#Adam optimizer is a common used optimizer\n",
    "#See https://keras.io/api/optimizers/adam/\n",
    "#See also https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e\n",
    "#SparseCategoricalCrossentropy loss function is being used because of the label format of the data\n",
    "#SparseCategoricalAccuracy is being used as metric because of the label format of the data\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A class inherited from keras utils Sequence is being created\n",
    "class FitSequence(tf.keras.utils.Sequence):\n",
    "    \n",
    "    #Constructor method is being defined\n",
    "    def __init__(self, image, label, batchSize):\n",
    "        self.image, self.label = image, label\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        #A numpy array for image indexes is being created\n",
    "        #This array will be used to shuffle the data\n",
    "        self.index = np.arange(self.image.shape[0])\n",
    "    \n",
    "    #__len__ method is being defined\n",
    "    #This method will be used by the model to show the amount of progress of each epoch\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.image.shape[0] / float(self.batchSize)))\n",
    "    \n",
    "    #__getitem__ method is being defined\n",
    "    #The model will retrieve the batches it will use during training by calling this method\n",
    "    #With this method, the data to be used by the model can be manipulated\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #When the model requests data, the next batch size will be selected based on index array\n",
    "        indexPart = self.index[idx * self.batchSize : (idx + 1) * self.batchSize]\n",
    "        \n",
    "        #Before being sent to the model on demand pixel values will be converted to range [-1, 1]\n",
    "        #Doing this operation here means that it will be repeated as many epochs for each image and this wastes time\n",
    "        #This is how the conversion is being done because of some memory problem in this project\n",
    "        batchX = (self.image[indexPart] / 127.5) - 1\n",
    "        batchY = self.label[indexPart]\n",
    "        return np.array(batchX), np.array(batchY)\n",
    "    \n",
    "    #on_epoch_end method is being defined\n",
    "    #The model will call this method after each epoch is ended\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        #At the end of the epoch, the index array is being shuffled \n",
    "        #so that the data in the next epoch is returned in different orders\n",
    "        np.random.shuffle(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "260/260 [==============================] - 283s 832ms/step - loss: 5.7603 - accuracy: 0.0796 - val_loss: 5.3389 - val_accuracy: 0.0952\n",
      "Epoch 2/35\n",
      "260/260 [==============================] - 213s 818ms/step - loss: 5.5246 - accuracy: 0.0852 - val_loss: 5.3304 - val_accuracy: 0.0952\n",
      "Epoch 3/35\n",
      "260/260 [==============================] - 220s 845ms/step - loss: 5.5066 - accuracy: 0.0852 - val_loss: 5.3139 - val_accuracy: 0.0952\n",
      "Epoch 4/35\n",
      "260/260 [==============================] - 251s 964ms/step - loss: 5.5027 - accuracy: 0.0852 - val_loss: 5.3503 - val_accuracy: 0.0952\n",
      "Epoch 5/35\n",
      "260/260 [==============================] - 251s 966ms/step - loss: 5.5022 - accuracy: 0.0852 - val_loss: 5.3116 - val_accuracy: 0.0952\n",
      "Epoch 6/35\n",
      "260/260 [==============================] - 250s 961ms/step - loss: 5.4978 - accuracy: 0.0852 - val_loss: 5.3044 - val_accuracy: 0.0952\n",
      "Epoch 7/35\n",
      "260/260 [==============================] - 239s 917ms/step - loss: 5.4944 - accuracy: 0.0852 - val_loss: 5.3148 - val_accuracy: 0.0952\n",
      "Epoch 8/35\n",
      "260/260 [==============================] - 239s 919ms/step - loss: 5.4948 - accuracy: 0.0852 - val_loss: 5.3064 - val_accuracy: 0.0952\n",
      "Epoch 9/35\n",
      "260/260 [==============================] - 239s 920ms/step - loss: 5.4938 - accuracy: 0.0852 - val_loss: 5.3024 - val_accuracy: 0.0952\n",
      "Epoch 10/35\n",
      "260/260 [==============================] - 247s 948ms/step - loss: 5.4931 - accuracy: 0.0852 - val_loss: 5.3030 - val_accuracy: 0.0952\n",
      "Epoch 11/35\n",
      "260/260 [==============================] - 257s 990ms/step - loss: 5.4900 - accuracy: 0.0852 - val_loss: 5.3250 - val_accuracy: 0.0952\n",
      "Epoch 12/35\n",
      "260/260 [==============================] - 257s 988ms/step - loss: 5.4928 - accuracy: 0.0852 - val_loss: 5.3185 - val_accuracy: 0.0952\n",
      "Epoch 13/35\n",
      "260/260 [==============================] - 257s 987ms/step - loss: 5.4905 - accuracy: 0.0852 - val_loss: 5.3117 - val_accuracy: 0.0952\n",
      "Epoch 14/35\n",
      "260/260 [==============================] - 250s 962ms/step - loss: 5.4919 - accuracy: 0.0852 - val_loss: 5.3293 - val_accuracy: 0.0952\n",
      "Epoch 15/35\n",
      "260/260 [==============================] - 241s 928ms/step - loss: 5.4879 - accuracy: 0.0852 - val_loss: 5.3171 - val_accuracy: 0.0952\n",
      "Epoch 16/35\n",
      "260/260 [==============================] - 231s 889ms/step - loss: 5.4883 - accuracy: 0.0852 - val_loss: 5.3116 - val_accuracy: 0.0952\n",
      "Epoch 17/35\n",
      "260/260 [==============================] - 251s 965ms/step - loss: 5.4849 - accuracy: 0.0852 - val_loss: 5.3220 - val_accuracy: 0.0952\n",
      "Epoch 18/35\n",
      "260/260 [==============================] - 251s 966ms/step - loss: 5.4848 - accuracy: 0.0852 - val_loss: 5.3226 - val_accuracy: 0.0952\n",
      "Epoch 19/35\n",
      "260/260 [==============================] - 252s 969ms/step - loss: 5.4832 - accuracy: 0.0852 - val_loss: 5.3059 - val_accuracy: 0.0952\n",
      "Epoch 20/35\n",
      "260/260 [==============================] - 251s 966ms/step - loss: 5.4840 - accuracy: 0.0852 - val_loss: 5.3147 - val_accuracy: 0.0952\n",
      "Epoch 21/35\n",
      "260/260 [==============================] - 251s 964ms/step - loss: 5.4844 - accuracy: 0.0852 - val_loss: 5.3061 - val_accuracy: 0.0952\n",
      "Epoch 22/35\n",
      "260/260 [==============================] - 239s 919ms/step - loss: 5.4837 - accuracy: 0.0852 - val_loss: 5.3265 - val_accuracy: 0.0952\n",
      "Epoch 23/35\n",
      "260/260 [==============================] - 239s 919ms/step - loss: 5.4828 - accuracy: 0.0852 - val_loss: 5.3209 - val_accuracy: 0.0952\n",
      "Epoch 24/35\n",
      "260/260 [==============================] - 248s 953ms/step - loss: 5.4821 - accuracy: 0.0852 - val_loss: 5.3187 - val_accuracy: 0.0952\n",
      "Epoch 25/35\n",
      "260/260 [==============================] - 290s 1s/step - loss: 5.4820 - accuracy: 0.0852 - val_loss: 5.3049 - val_accuracy: 0.0952\n",
      "Epoch 26/35\n",
      "260/260 [==============================] - 250s 962ms/step - loss: 5.4854 - accuracy: 0.0852 - val_loss: 5.3060 - val_accuracy: 0.0952\n",
      "Epoch 27/35\n",
      "260/260 [==============================] - 248s 954ms/step - loss: 5.4818 - accuracy: 0.0852 - val_loss: 5.3213 - val_accuracy: 0.0952\n",
      "Epoch 28/35\n",
      "260/260 [==============================] - 238s 916ms/step - loss: 5.4794 - accuracy: 0.0852 - val_loss: 5.3109 - val_accuracy: 0.0952\n",
      "Epoch 29/35\n",
      "260/260 [==============================] - 238s 916ms/step - loss: 5.4825 - accuracy: 0.0852 - val_loss: 5.3159 - val_accuracy: 0.0952\n",
      "Epoch 30/35\n",
      "260/260 [==============================] - 243s 934ms/step - loss: 5.4816 - accuracy: 0.0852 - val_loss: 5.3113 - val_accuracy: 0.0952\n",
      "Epoch 31/35\n",
      "260/260 [==============================] - 251s 966ms/step - loss: 5.4800 - accuracy: 0.0852 - val_loss: 5.3138 - val_accuracy: 0.0952\n",
      "Epoch 32/35\n",
      "260/260 [==============================] - 252s 967ms/step - loss: 5.4793 - accuracy: 0.0852 - val_loss: 5.3059 - val_accuracy: 0.0952\n",
      "Epoch 33/35\n",
      "260/260 [==============================] - 255s 981ms/step - loss: 5.4809 - accuracy: 0.0852 - val_loss: 5.3127 - val_accuracy: 0.0952\n",
      "Epoch 34/35\n",
      "260/260 [==============================] - 252s 968ms/step - loss: 5.4805 - accuracy: 0.0852 - val_loss: 5.3019 - val_accuracy: 0.0952\n",
      "Epoch 35/35\n",
      "260/260 [==============================] - 238s 914ms/step - loss: 5.4784 - accuracy: 0.0852 - val_loss: 5.3023 - val_accuracy: 0.0952\n"
     ]
    }
   ],
   "source": [
    "#model is being trained with 35 epochs and 16 batchSize using GPU\n",
    "#A small batchSize value is being chosen to prevent GPU memory problem\n",
    "#Large batchSize reduce training time while also generally providing better results\n",
    "with tf.device(myGPU):\n",
    "    trainingHistory = model.fit(\n",
    "        FitSequence(trainingX, trainingY, 16),\n",
    "        epochs = 35,\n",
    "        validation_data = FitSequence(validationX, validationY, 16)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Chart is being drawn\n",
    "\n",
    "#Chart Values\n",
    "plt.plot(trainingHistory.history['loss'])\n",
    "plt.plot(trainingHistory.history['val_loss'])\n",
    "\n",
    "#Chart Tittle\n",
    "plt.title('Model Loss Chart')\n",
    "\n",
    "#Chart Labels\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#Chart Lines\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "#Show Method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Chart is being drawn\n",
    "\n",
    "#Chart Values\n",
    "plt.plot(trainingHistory.history['accuracy'])\n",
    "plt.plot(trainingHistory.history['val_accuracy'])\n",
    "\n",
    "#Chart Tittle\n",
    "plt.title('model accuracy')\n",
    "\n",
    "#Chart Labels\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#Chart Lines\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "#Show Method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270</td>\n",
       "      <td>[[[0, 0, 3], [0, 0, 3], [0, 0, 3], [0, 0, 3], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>[[[100, 142, 119], [100, 142, 119], [100, 142,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[166, 148, 131], [167, 146, 129], [159, 136,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401</td>\n",
       "      <td>[[[8, 0, 0], [8, 0, 0], [8, 0, 0], [8, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>291</td>\n",
       "      <td>[[[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>302</td>\n",
       "      <td>[[[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>401</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>340</td>\n",
       "      <td>[[[9, 32, 77], [9, 32, 77], [9, 32, 77], [10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>293</td>\n",
       "      <td>[[[96, 12, 71], [96, 12, 71], [97, 13, 72], [9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PersonID                                           ImageBGR\n",
       "0         270  [[[0, 0, 3], [0, 0, 3], [0, 0, 3], [0, 0, 3], ...\n",
       "1          80  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n",
       "2          14  [[[100, 142, 119], [100, 142, 119], [100, 142,...\n",
       "3         120  [[[166, 148, 131], [167, 146, 129], [159, 136,...\n",
       "4         401  [[[8, 0, 0], [8, 0, 0], [8, 0, 0], [8, 0, 0], ...\n",
       "..        ...                                                ...\n",
       "910       291  [[[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], ...\n",
       "911       302  [[[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], ...\n",
       "912       401  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n",
       "913       340  [[[9, 32, 77], [9, 32, 77], [9, 32, 77], [10, ...\n",
       "914       293  [[[96, 12, 71], [96, 12, 71], [97, 13, 72], [9...\n",
       "\n",
       "[915 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FullPhoto Test data is being read from md5 file\n",
    "testDf = pd.read_pickle(\"../../../Data/ResizedData/FullPhoto/Test.pkl\")\n",
    "testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testX is being extracted from testDf as wanted shape\n",
    "testX = np.array(testDf.ImageBGR.values.tolist())\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testY is being extracted from testDf as wanted shape\n",
    "testY = np.array(testDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 31s 536ms/step - loss: 5.3381 - accuracy: 0.0973\n"
     ]
    }
   ],
   "source": [
    "#Model is being evaluated with test data\n",
    "#Sequence class is being also used for evaluation to convert test data into the same format as training data\n",
    "testResult = model.evaluate(FitSequence(testX, testY, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.338145732879639\n"
     ]
    }
   ],
   "source": [
    "#Test Loss is being Printed\n",
    "print('Test Loss: ' + str(testResult[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0972677618265152\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy is being Printed\n",
    "print('Test Accuracy: ' + str(testResult[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training9 Inference\n",
    "\n",
    "By looking at the charts, it can be seen that learning does not take place.\n",
    "\n",
    "The model has not learned enough to have any success even on the Training data, even overfitting did not occur.\n",
    "\n",
    "It can even be seen that the accuracy values are stuck and do not change throughout the training.\n",
    "\n",
    "Performance can be improved by trying Hyperparameter Optimization methods.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Hyperparameter_optimization.\n",
    "\n",
    "Since there is no expectation from this unprocessed dataset, which is already imbalanced.\n",
    "\n",
    "This training will not be focussed on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39AI",
   "language": "python",
   "name": "py39ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
