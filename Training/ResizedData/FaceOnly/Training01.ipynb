{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training1\n",
    "\n",
    "In this notebook file, ResizedData-FaceOnly dataset will be read from pkl file.\n",
    "\n",
    "Input(X) and Output(Y) numpy arrays will be created from pandas dataframes.\n",
    "\n",
    "The VGG16 model will be loaded with random weights so only the architecture of the VGG16 model will be used for training.\n",
    "\n",
    "A keras utils Sequence class will be defined so that operations can be performed on the data to be used during the training.\n",
    "\n",
    "Performance will be checked with Validation data while training model with Training data.\n",
    "\n",
    "Accuracy and Loss charts will be drawn according to epoch numbers.\n",
    "\n",
    "The results obtained by evaluating the model with Test data will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries are being imported\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.22.3\n",
      "pandas Version: 1.4.3\n",
      "tensorflow Version: 2.6.0\n",
      "matplotlib Version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "#Library versions are being printed\n",
    "print('numpy Version: ' + np.__version__)\n",
    "print('pandas Version: ' + pd.__version__)\n",
    "print('tensorflow Version: ' + tf.__version__)\n",
    "print('matplotlib Version: ' + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#GPU will be used for training\n",
    "myGPU = tf.test.gpu_device_name()\n",
    "if myGPU:\n",
    "    print(myGPU)\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abdullah Gul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrien Brody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmed Chalabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ai Sugiyama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Greenspan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Yasser Arafat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Yoko Ono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Yoriko Kawaguchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Zhu Rongji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Zinedine Zidane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name\n",
       "ID                   \n",
       "0        Abdullah Gul\n",
       "1        Adrien Brody\n",
       "2       Ahmed Chalabi\n",
       "3         Ai Sugiyama\n",
       "4      Alan Greenspan\n",
       "..                ...\n",
       "418     Yasser Arafat\n",
       "419          Yoko Ono\n",
       "420  Yoriko Kawaguchi\n",
       "421        Zhu Rongji\n",
       "422   Zinedine Zidane\n",
       "\n",
       "[423 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Person dataframe in ResizedData is being read from md5 file\n",
    "personDf = pd.read_pickle(\"../../../Data/ResizedData/Person.pkl\")\n",
    "personDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "      <th>DetectionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>[[[71, 116, 99], [69, 116, 98], [67, 115, 98],...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>[[[10, 24, 36], [12, 26, 38], [18, 32, 44], [2...</td>\n",
       "      <td>NoFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356</td>\n",
       "      <td>[[[177, 199, 204], [176, 199, 204], [175, 200,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>277</td>\n",
       "      <td>[[[91, 103, 121], [91, 104, 122], [92, 105, 12...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131</td>\n",
       "      <td>[[[42, 65, 81], [38, 61, 77], [30, 53, 68], [2...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[64, 89, 93], [62, 88, 92], [60, 86, 90], [5...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[114, 93, 71], [116, 96, 74], [122, 101, 79]...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>322</td>\n",
       "      <td>[[[197, 207, 207], [196, 208, 210], [195, 207,...</td>\n",
       "      <td>NoFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>383</td>\n",
       "      <td>[[[7, 5, 5], [7, 5, 5], [8, 5, 5], [8, 6, 6], ...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>44</td>\n",
       "      <td>[[[6, 4, 4], [6, 4, 4], [5, 5, 5], [5, 5, 5], ...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PersonID                                           ImageBGR  \\\n",
       "0           22  [[[71, 116, 99], [69, 116, 98], [67, 115, 98],...   \n",
       "1          125  [[[10, 24, 36], [12, 26, 38], [18, 32, 44], [2...   \n",
       "2          356  [[[177, 199, 204], [176, 199, 204], [175, 200,...   \n",
       "3          277  [[[91, 103, 121], [91, 104, 122], [92, 105, 12...   \n",
       "4          131  [[[42, 65, 81], [38, 61, 77], [30, 53, 68], [2...   \n",
       "...        ...                                                ...   \n",
       "4151         4  [[[64, 89, 93], [62, 88, 92], [60, 86, 90], [5...   \n",
       "4152       120  [[[114, 93, 71], [116, 96, 74], [122, 101, 79]...   \n",
       "4153       322  [[[197, 207, 207], [196, 208, 210], [195, 207,...   \n",
       "4154       383  [[[7, 5, 5], [7, 5, 5], [8, 5, 5], [8, 6, 6], ...   \n",
       "4155        44  [[[6, 4, 4], [6, 4, 4], [5, 5, 5], [5, 5, 5], ...   \n",
       "\n",
       "     DetectionType  \n",
       "0       SingleFace  \n",
       "1           NoFace  \n",
       "2       SingleFace  \n",
       "3       SingleFace  \n",
       "4       SingleFace  \n",
       "...            ...  \n",
       "4151    SingleFace  \n",
       "4152    SingleFace  \n",
       "4153        NoFace  \n",
       "4154    SingleFace  \n",
       "4155    SingleFace  \n",
       "\n",
       "[4156 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FaceOnly Training data is being read from md5 file\n",
    "trainingDf = pd.read_pickle(\"../../../Data/ResizedData/FaceOnly/Training.pkl\")\n",
    "trainingDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainingX is being extracted from trainingDf as wanted shape\n",
    "#trainingX contains images with pixel values of data type np.uint8 in the range [0, 255]\n",
    "\n",
    "#Many pre-trained models, including the ones to be used within the scope of this project,\n",
    "#have been trained with images containing pixel values in the [-1, 1] range\n",
    "#In this way, the data will be symmetrical and the performance of the Backpropagation algorithm will be increased\n",
    "#See https://en.wikipedia.org/wiki/Backpropagation\n",
    "#See also https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data\n",
    "#Therefore, training will be performed by converting pixel values to this range with the simplest method (pixel / 127.5 - 1)\n",
    "\n",
    "#Converting pixel values to range [-1, 1] in this section is an option\n",
    "#Doing this once over the entire array now will be save time\n",
    "#This is not how the conversion will be done because of some memory problems in this project\n",
    "#Images are of data type np.uint8 when they are in the range [0, 255]\n",
    "#np.uint8 requires 1 byte memory while np.float32 requires 4 byte and np.float64 requires 8 byte\n",
    "#See https://www.educba.com/numpy-data-types/\n",
    "#When np.uint8 data type, images use about 1GB memory\n",
    "#Even if these pixel values are converted to np.float32 data type, it will need about 4GB of memory\n",
    "#The computer used for this project has 8GB Ram\n",
    "#Considering operating system requirements, memory required by the model, etc. 8GB Ram is not enough for this process\n",
    "#For this reason, this method is not preferred, although it will save time\n",
    "\n",
    "trainingX = np.array(trainingDf.ImageBGR.values.tolist())\n",
    "trainingX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainingY is being extracted from trainingDf as wanted shape\n",
    "trainingY = np.array(trainingDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "trainingY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "      <th>DetectionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>[[[64, 65, 56], [93, 94, 85], [143, 144, 135],...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[102, 116, 110], [106, 120, 114], [115, 129,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196</td>\n",
       "      <td>[[[23, 40, 43], [23, 40, 43], [24, 41, 44], [2...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95</td>\n",
       "      <td>[[[34, 55, 63], [35, 56, 65], [38, 59, 69], [4...</td>\n",
       "      <td>MultipleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>380</td>\n",
       "      <td>[[[227, 227, 227], [227, 227, 227], [227, 227,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>71</td>\n",
       "      <td>[[[104, 116, 120], [103, 116, 121], [102, 117,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>183</td>\n",
       "      <td>[[[35, 17, 10], [35, 17, 11], [36, 17, 12], [3...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[109, 141, 160], [105, 137, 156], [97, 129, ...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[84, 94, 111], [81, 91, 108], [76, 86, 103],...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>209</td>\n",
       "      <td>[[[59, 64, 65], [57, 61, 62], [53, 57, 58], [5...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>914 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PersonID                                           ImageBGR DetectionType\n",
       "0         171  [[[64, 65, 56], [93, 94, 85], [143, 144, 135],...    SingleFace\n",
       "1         120  [[[102, 116, 110], [106, 120, 114], [115, 129,...    SingleFace\n",
       "2         196  [[[23, 40, 43], [23, 40, 43], [24, 41, 44], [2...    SingleFace\n",
       "3          95  [[[34, 55, 63], [35, 56, 65], [38, 59, 69], [4...  MultipleFace\n",
       "4         380  [[[227, 227, 227], [227, 227, 227], [227, 227,...    SingleFace\n",
       "..        ...                                                ...           ...\n",
       "909        71  [[[104, 116, 120], [103, 116, 121], [102, 117,...    SingleFace\n",
       "910       183  [[[35, 17, 10], [35, 17, 11], [36, 17, 12], [3...    SingleFace\n",
       "911       120  [[[109, 141, 160], [105, 137, 156], [97, 129, ...    SingleFace\n",
       "912       120  [[[84, 94, 111], [81, 91, 108], [76, 86, 103],...    SingleFace\n",
       "913       209  [[[59, 64, 65], [57, 61, 62], [53, 57, 58], [5...    SingleFace\n",
       "\n",
       "[914 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FaceOnly Validation data is being read from md5 file\n",
    "validationDf = pd.read_pickle(\"../../../Data/ResizedData/FaceOnly/Validation.pkl\")\n",
    "validationDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validationX is being extracted from validationDf as wanted shape\n",
    "#validationX contains images with pixel values of data type np.uint8 in the range [0, 255]\n",
    "\n",
    "#Many pre-trained models, including the ones to be used within the scope of this project,\n",
    "#have been trained with images containing pixel values in the [-1, 1] range\n",
    "#In this way, the data will be symmetrical and the performance of the Backpropagation algorithm will be increased\n",
    "#See https://en.wikipedia.org/wiki/Backpropagation\n",
    "#See also https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data\n",
    "#Therefore, training will be performed by converting pixel values to this range with the simplest method (pixel / 127.5 - 1)\n",
    "\n",
    "#Converting pixel values to range [-1, 1] in this section is an option\n",
    "#Doing this once over the entire array now will be save time\n",
    "#This is not how the conversion will be done because of some memory problems in this project\n",
    "#Images are of data type np.uint8 when they are in the range [0, 255]\n",
    "#np.uint8 requires 1 byte memory while np.float32 requires 4 byte and np.float64 requires 8 byte\n",
    "#See https://www.educba.com/numpy-data-types/\n",
    "#When np.uint8 data type, images use about 1GB memory\n",
    "#Even if these pixel values are converted to np.float32 data type, it will need about 4GB of memory\n",
    "#The computer used for this project has 8GB Ram\n",
    "#Considering operating system requirements, memory required by the model, etc. 8GB Ram is not enough for this process\n",
    "#For this reason, this method is not preferred, although it will save time\n",
    "\n",
    "validationX = np.array(validationDf.ImageBGR.values.tolist())\n",
    "validationX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validationY is being extracted from validationDf as wanted shape\n",
    "validationY = np.array(validationDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "validationY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VGG16 model with the None weights parameter is being load\n",
    "#Thus, architecture of the VGG16 model is being load with random weights\n",
    "#This way the model can be trained from scratch\n",
    "\n",
    "#The original VGG16 pre-trained model was trained with images with size of (224, 224, 3) \n",
    "#in BGR color order and pixel values of [-1, 1] (zero centered) as default\n",
    "#See https://keras.io/api/applications/vgg/ for more information\n",
    "\n",
    "#Since only the architecture of the VGG16 model is being loaded, the loaded model is not a pre-trained model\n",
    "#Therefore, training can be performed regardless of the data type of the VGG16 pre-trained model\n",
    "\n",
    "#But since images of dataset saved as size of (224, 224, 3) in BGR color order and pixel values of [0, 255]\n",
    "#And pixel values will be converted to [-1, 1] range during training with the help of keras utils Sequence class\n",
    "#to improve the performance of the Backpropagation algorithm\n",
    "#The dataset will be used exactly in the format in which the original VGG16 pre-trained model was trained\n",
    "\n",
    "model = tf.keras.applications.vgg16.VGG16(include_top = False, weights = None, input_shape = ((224, 224, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 423)               433575    \n",
      "=================================================================\n",
      "Total params: 41,888,999\n",
      "Trainable params: 41,888,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#The architecture of VGG16 model is being connected to the fully connected layer\n",
    "#A dropout layer is being added to the the model to prevent overfitting,\n",
    "#and the model is being completed with the addition of the output layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(personDf.shape[0], activation = tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model is being compiled with Adam optimizer\n",
    "#Adam optimizer is a common used optimizer\n",
    "#See https://keras.io/api/optimizers/adam/\n",
    "#See also https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e\n",
    "#SparseCategoricalCrossentropy loss function is being used because of the label format of the data\n",
    "#SparseCategoricalAccuracy is being used as metric because of the label format of the data\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A class inherited from keras utils Sequence is being created\n",
    "class FitSequence(tf.keras.utils.Sequence):\n",
    "    \n",
    "    #Constructor method is being defined\n",
    "    def __init__(self, image, label, batchSize):\n",
    "        self.image, self.label = image, label\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        #A numpy array for image indexes is being created\n",
    "        #This array will be used to shuffle the data\n",
    "        self.index = np.arange(self.image.shape[0])\n",
    "    \n",
    "    #__len__ method is being defined\n",
    "    #This method will be used by the model to show the amount of progress of each epoch\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.image.shape[0] / float(self.batchSize)))\n",
    "    \n",
    "    #__getitem__ method is being defined\n",
    "    #The model will retrieve the batches it will use during training by calling this method\n",
    "    #With this method, the data to be used by the model can be manipulated\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #When the model requests data, the next batch size will be selected based on index array\n",
    "        indexPart = self.index[idx * self.batchSize : (idx + 1) * self.batchSize]\n",
    "        \n",
    "        #Before being sent to the model on demand pixel values will be converted to range [-1, 1]\n",
    "        #Doing this operation here means that it will be repeated as many epochs for each image and this wastes time\n",
    "        #This is how the conversion is being done because of some memory problem in this project\n",
    "        batchX = (self.image[indexPart] / 127.5) - 1\n",
    "        batchY = self.label[indexPart]\n",
    "        return np.array(batchX), np.array(batchY)\n",
    "    \n",
    "    #on_epoch_end method is being defined\n",
    "    #The model will call this method after each epoch is ended\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        #At the end of the epoch, the index array is being shuffled \n",
    "        #so that the data in the next epoch is returned in different orders\n",
    "        np.random.shuffle(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "520/520 [==============================] - 672s 1s/step - loss: 5.7150 - accuracy: 0.0830 - val_loss: 5.3472 - val_accuracy: 0.0952\n",
      "Epoch 2/15\n",
      "520/520 [==============================] - 649s 1s/step - loss: 5.5132 - accuracy: 0.0852 - val_loss: 5.3075 - val_accuracy: 0.0952\n",
      "Epoch 3/15\n",
      "520/520 [==============================] - 663s 1s/step - loss: 5.5047 - accuracy: 0.0852 - val_loss: 5.3122 - val_accuracy: 0.0952\n",
      "Epoch 4/15\n",
      "520/520 [==============================] - 657s 1s/step - loss: 5.5005 - accuracy: 0.0852 - val_loss: 5.3152 - val_accuracy: 0.0952\n",
      "Epoch 5/15\n",
      "520/520 [==============================] - 663s 1s/step - loss: 5.4992 - accuracy: 0.0852 - val_loss: 5.3125 - val_accuracy: 0.0952\n",
      "Epoch 6/15\n",
      "520/520 [==============================] - 676s 1s/step - loss: 5.4942 - accuracy: 0.0852 - val_loss: 5.3122 - val_accuracy: 0.0952\n",
      "Epoch 7/15\n",
      "520/520 [==============================] - 697s 1s/step - loss: 5.4930 - accuracy: 0.0852 - val_loss: 5.3232 - val_accuracy: 0.0952\n",
      "Epoch 8/15\n",
      "520/520 [==============================] - 773s 1s/step - loss: 5.4921 - accuracy: 0.0852 - val_loss: 5.3086 - val_accuracy: 0.0952\n",
      "Epoch 9/15\n",
      "520/520 [==============================] - 705s 1s/step - loss: 5.4923 - accuracy: 0.0852 - val_loss: 5.3170 - val_accuracy: 0.0952\n",
      "Epoch 10/15\n",
      "520/520 [==============================] - 666s 1s/step - loss: 5.4898 - accuracy: 0.0852 - val_loss: 5.3225 - val_accuracy: 0.0952\n",
      "Epoch 11/15\n",
      "520/520 [==============================] - 707s 1s/step - loss: 5.4900 - accuracy: 0.0852 - val_loss: 5.3143 - val_accuracy: 0.0952\n",
      "Epoch 12/15\n",
      "520/520 [==============================] - 666s 1s/step - loss: 5.4881 - accuracy: 0.0852 - val_loss: 5.3250 - val_accuracy: 0.0952\n",
      "Epoch 13/15\n",
      "520/520 [==============================] - 646s 1s/step - loss: 5.4874 - accuracy: 0.0852 - val_loss: 5.3130 - val_accuracy: 0.0952\n",
      "Epoch 14/15\n",
      "520/520 [==============================] - 702s 1s/step - loss: 5.4890 - accuracy: 0.0852 - val_loss: 5.3075 - val_accuracy: 0.0952\n",
      "Epoch 15/15\n",
      "520/520 [==============================] - 672s 1s/step - loss: 5.4852 - accuracy: 0.0852 - val_loss: 5.3080 - val_accuracy: 0.0952\n"
     ]
    }
   ],
   "source": [
    "#model is being trained with 15 epochs and 8 batchSize using GPU\n",
    "#A small batchSize value is being chosen to prevent GPU memory problem\n",
    "#Large batchSize reduce training time while also generally providing better results\n",
    "with tf.device(myGPU):\n",
    "    trainingHistory = model.fit(\n",
    "        FitSequence(trainingX, trainingY, 8),\n",
    "        epochs = 15,\n",
    "        validation_data = FitSequence(validationX, validationY, 8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Chart is being drawn\n",
    "\n",
    "#Chart Values\n",
    "plt.plot(trainingHistory.history['loss'])\n",
    "plt.plot(trainingHistory.history['val_loss'])\n",
    "\n",
    "#Chart Tittle\n",
    "plt.title('Model Loss Chart')\n",
    "\n",
    "#Chart Labels\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#Chart Lines\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "#Show Method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Chart is being drawn\n",
    "\n",
    "#Chart Values\n",
    "plt.plot(trainingHistory.history['accuracy'])\n",
    "plt.plot(trainingHistory.history['val_accuracy'])\n",
    "\n",
    "#Chart Tittle\n",
    "plt.title('model accuracy')\n",
    "\n",
    "#Chart Labels\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#Chart Lines\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "#Show Method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "      <th>DetectionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270</td>\n",
       "      <td>[[[0, 2, 13], [0, 1, 11], [0, 0, 8], [0, 0, 7]...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>[[[9, 11, 22], [9, 11, 23], [10, 12, 25], [11,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>[[[43, 69, 53], [32, 58, 42], [13, 37, 22], [6...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[161, 124, 104], [160, 124, 105], [159, 125,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401</td>\n",
       "      <td>[[[8, 15, 142], [7, 15, 142], [6, 15, 142], [6...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>291</td>\n",
       "      <td>[[[157, 175, 176], [152, 170, 173], [143, 161,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>302</td>\n",
       "      <td>[[[76, 82, 89], [76, 82, 89], [77, 82, 90], [7...</td>\n",
       "      <td>MultipleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>401</td>\n",
       "      <td>[[[39, 42, 46], [38, 41, 45], [37, 40, 44], [3...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>340</td>\n",
       "      <td>[[[11, 16, 25], [11, 16, 24], [12, 15, 23], [1...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>293</td>\n",
       "      <td>[[[154, 178, 202], [155, 179, 203], [157, 181,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PersonID                                           ImageBGR DetectionType\n",
       "0         270  [[[0, 2, 13], [0, 1, 11], [0, 0, 8], [0, 0, 7]...    SingleFace\n",
       "1          80  [[[9, 11, 22], [9, 11, 23], [10, 12, 25], [11,...    SingleFace\n",
       "2          14  [[[43, 69, 53], [32, 58, 42], [13, 37, 22], [6...    SingleFace\n",
       "3         120  [[[161, 124, 104], [160, 124, 105], [159, 125,...    SingleFace\n",
       "4         401  [[[8, 15, 142], [7, 15, 142], [6, 15, 142], [6...    SingleFace\n",
       "..        ...                                                ...           ...\n",
       "910       291  [[[157, 175, 176], [152, 170, 173], [143, 161,...    SingleFace\n",
       "911       302  [[[76, 82, 89], [76, 82, 89], [77, 82, 90], [7...  MultipleFace\n",
       "912       401  [[[39, 42, 46], [38, 41, 45], [37, 40, 44], [3...    SingleFace\n",
       "913       340  [[[11, 16, 25], [11, 16, 24], [12, 15, 23], [1...    SingleFace\n",
       "914       293  [[[154, 178, 202], [155, 179, 203], [157, 181,...    SingleFace\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FaceOnly Test data is being read from md5 file\n",
    "testDf = pd.read_pickle(\"../../../Data/ResizedData/FaceOnly/Test.pkl\")\n",
    "testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 224, 224, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testX is being extracted from testDf as wanted shape\n",
    "testX = np.array(testDf.ImageBGR.values.tolist())\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testY is being extracted from testDf as wanted shape\n",
    "testY = np.array(testDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 42s 517ms/step - loss: 5.3425 - accuracy: 0.0973\n"
     ]
    }
   ],
   "source": [
    "#Model is being evaluated with test data\n",
    "#Sequence class is being also used for evaluation to convert test data into the same format as training data\n",
    "testResult = model.evaluate(FitSequence(testX, testY, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.342526435852051\n"
     ]
    }
   ],
   "source": [
    "#Test Loss is being Printed\n",
    "print('Test Loss: ' + str(testResult[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0972677618265152\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy is being Printed\n",
    "print('Test Accuracy: ' + str(testResult[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training1 Inference\n",
    "\n",
    "By looking at the charts, it can be seen that learning does not take place.\n",
    "\n",
    "The model has not learned enough to have any success even on the Training data, even overfitting did not occur.\n",
    "\n",
    "Performance can be improved by trying Hyperparameter Optimization methods.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Hyperparameter_optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39AI",
   "language": "python",
   "name": "py39ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
