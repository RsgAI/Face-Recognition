{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training9\n",
    "\n",
    "In this notebook file, ResizedData-FaceOnly dataset will be read from pkl file.\n",
    "\n",
    "Input(X) and Output(Y) numpy arrays will be created from pandas dataframes.\n",
    "\n",
    "VGG16 pre-trained model will be load and used.\n",
    "\n",
    "Pre-trained model's layers except the last CNN block will be set to non-trainable.\n",
    "\n",
    "Training will be performed in the sections between the last CNN block and the Output layer.\n",
    "\n",
    "In this way, the experience gained by the model on very large datasets will be used in this classification problem, while the last CNN block will be updated and fine-tuned.\n",
    "\n",
    "This method is known as [**Fine-Tuning**](https://deeplizard.com/learn/video/5T-iXNNiwIs \"deeplizard\").\n",
    "\n",
    "See also [**Transfer Learning and Fine-Tuning**](https://www.tensorflow.org/tutorials/images/transfer_learning \"tensorflow\").\n",
    "\n",
    "A keras utils Sequence class will be defined so that operations can be performed on the data to be used during the training.\n",
    "\n",
    "Performance will be checked with Validation data while training model with Training data.\n",
    "\n",
    "Accuracy and Loss charts will be drawn according to epoch numbers.\n",
    "\n",
    "The results obtained by evaluating the model with Test data will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries are being imported\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy Version: 1.22.3\n",
      "pandas Version: 1.4.3\n",
      "tensorflow Version: 2.6.0\n",
      "matplotlib Version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "#Library versions are being printed\n",
    "print('numpy Version: ' + np.__version__)\n",
    "print('pandas Version: ' + pd.__version__)\n",
    "print('tensorflow Version: ' + tf.__version__)\n",
    "print('matplotlib Version: ' + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#GPU will be used for training\n",
    "myGPU = tf.test.gpu_device_name()\n",
    "if myGPU:\n",
    "    print(myGPU)\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abdullah Gul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrien Brody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmed Chalabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ai Sugiyama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Greenspan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Yasser Arafat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Yoko Ono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Yoriko Kawaguchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Zhu Rongji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Zinedine Zidane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name\n",
       "ID                   \n",
       "0        Abdullah Gul\n",
       "1        Adrien Brody\n",
       "2       Ahmed Chalabi\n",
       "3         Ai Sugiyama\n",
       "4      Alan Greenspan\n",
       "..                ...\n",
       "418     Yasser Arafat\n",
       "419          Yoko Ono\n",
       "420  Yoriko Kawaguchi\n",
       "421        Zhu Rongji\n",
       "422   Zinedine Zidane\n",
       "\n",
       "[423 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Person dataframe in ResizedData is being read from md5 file\n",
    "personDf = pd.read_pickle(\"../../../Data/ResizedData/Person.pkl\")\n",
    "personDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "      <th>DetectionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>[[[71, 116, 99], [69, 116, 98], [67, 115, 98],...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>[[[10, 24, 36], [12, 26, 38], [18, 32, 44], [2...</td>\n",
       "      <td>NoFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356</td>\n",
       "      <td>[[[177, 199, 204], [176, 199, 204], [175, 200,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>277</td>\n",
       "      <td>[[[91, 103, 121], [91, 104, 122], [92, 105, 12...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131</td>\n",
       "      <td>[[[42, 65, 81], [38, 61, 77], [30, 53, 68], [2...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[64, 89, 93], [62, 88, 92], [60, 86, 90], [5...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[114, 93, 71], [116, 96, 74], [122, 101, 79]...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>322</td>\n",
       "      <td>[[[197, 207, 207], [196, 208, 210], [195, 207,...</td>\n",
       "      <td>NoFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>383</td>\n",
       "      <td>[[[7, 5, 5], [7, 5, 5], [8, 5, 5], [8, 6, 6], ...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>44</td>\n",
       "      <td>[[[6, 4, 4], [6, 4, 4], [5, 5, 5], [5, 5, 5], ...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PersonID                                           ImageBGR  \\\n",
       "0           22  [[[71, 116, 99], [69, 116, 98], [67, 115, 98],...   \n",
       "1          125  [[[10, 24, 36], [12, 26, 38], [18, 32, 44], [2...   \n",
       "2          356  [[[177, 199, 204], [176, 199, 204], [175, 200,...   \n",
       "3          277  [[[91, 103, 121], [91, 104, 122], [92, 105, 12...   \n",
       "4          131  [[[42, 65, 81], [38, 61, 77], [30, 53, 68], [2...   \n",
       "...        ...                                                ...   \n",
       "4151         4  [[[64, 89, 93], [62, 88, 92], [60, 86, 90], [5...   \n",
       "4152       120  [[[114, 93, 71], [116, 96, 74], [122, 101, 79]...   \n",
       "4153       322  [[[197, 207, 207], [196, 208, 210], [195, 207,...   \n",
       "4154       383  [[[7, 5, 5], [7, 5, 5], [8, 5, 5], [8, 6, 6], ...   \n",
       "4155        44  [[[6, 4, 4], [6, 4, 4], [5, 5, 5], [5, 5, 5], ...   \n",
       "\n",
       "     DetectionType  \n",
       "0       SingleFace  \n",
       "1           NoFace  \n",
       "2       SingleFace  \n",
       "3       SingleFace  \n",
       "4       SingleFace  \n",
       "...            ...  \n",
       "4151    SingleFace  \n",
       "4152    SingleFace  \n",
       "4153        NoFace  \n",
       "4154    SingleFace  \n",
       "4155    SingleFace  \n",
       "\n",
       "[4156 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FaceOnly Training data is being read from md5 file\n",
    "trainingDf = pd.read_pickle(\"../../../Data/ResizedData/FaceOnly/Training.pkl\")\n",
    "trainingDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainingX is being extracted from trainingDf as wanted shape\n",
    "#trainingX contains images with pixel values of data type np.uint8 in the range [0, 255]\n",
    "\n",
    "#Many pre-trained models, including the ones to be used within the scope of this project,\n",
    "#have been trained with images containing pixel values in the [-1, 1] range\n",
    "#In this way, the data will be symmetrical and the performance of the Backpropagation algorithm will be increased\n",
    "#See https://en.wikipedia.org/wiki/Backpropagation\n",
    "#See also https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data\n",
    "#Therefore, training will be performed by converting pixel values to this range with the simplest method (pixel / 127.5 - 1)\n",
    "\n",
    "#Converting pixel values to range [-1, 1] in this section is an option\n",
    "#Doing this once over the entire array now will be save time\n",
    "#This is not how the conversion will be done because of some memory problems in this project\n",
    "#Images are of data type np.uint8 when they are in the range [0, 255]\n",
    "#np.uint8 requires 1 byte memory while np.float32 requires 4 byte and np.float64 requires 8 byte\n",
    "#See https://www.educba.com/numpy-data-types/\n",
    "#When np.uint8 data type, images use about 1GB memory\n",
    "#Even if these pixel values are converted to np.float32 data type, it will need about 4GB of memory\n",
    "#The computer used for this project has 8GB Ram\n",
    "#Considering operating system requirements, memory required by the model, etc. 8GB Ram is not enough for this process\n",
    "#For this reason, this method is not preferred, although it will save time\n",
    "\n",
    "trainingX = np.array(trainingDf.ImageBGR.values.tolist())\n",
    "trainingX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainingY is being extracted from trainingDf as wanted shape\n",
    "trainingY = np.array(trainingDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "trainingY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "      <th>DetectionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>[[[64, 65, 56], [93, 94, 85], [143, 144, 135],...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[102, 116, 110], [106, 120, 114], [115, 129,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196</td>\n",
       "      <td>[[[23, 40, 43], [23, 40, 43], [24, 41, 44], [2...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95</td>\n",
       "      <td>[[[34, 55, 63], [35, 56, 65], [38, 59, 69], [4...</td>\n",
       "      <td>MultipleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>380</td>\n",
       "      <td>[[[227, 227, 227], [227, 227, 227], [227, 227,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>71</td>\n",
       "      <td>[[[104, 116, 120], [103, 116, 121], [102, 117,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>183</td>\n",
       "      <td>[[[35, 17, 10], [35, 17, 11], [36, 17, 12], [3...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[109, 141, 160], [105, 137, 156], [97, 129, ...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[84, 94, 111], [81, 91, 108], [76, 86, 103],...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>209</td>\n",
       "      <td>[[[59, 64, 65], [57, 61, 62], [53, 57, 58], [5...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>914 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PersonID                                           ImageBGR DetectionType\n",
       "0         171  [[[64, 65, 56], [93, 94, 85], [143, 144, 135],...    SingleFace\n",
       "1         120  [[[102, 116, 110], [106, 120, 114], [115, 129,...    SingleFace\n",
       "2         196  [[[23, 40, 43], [23, 40, 43], [24, 41, 44], [2...    SingleFace\n",
       "3          95  [[[34, 55, 63], [35, 56, 65], [38, 59, 69], [4...  MultipleFace\n",
       "4         380  [[[227, 227, 227], [227, 227, 227], [227, 227,...    SingleFace\n",
       "..        ...                                                ...           ...\n",
       "909        71  [[[104, 116, 120], [103, 116, 121], [102, 117,...    SingleFace\n",
       "910       183  [[[35, 17, 10], [35, 17, 11], [36, 17, 12], [3...    SingleFace\n",
       "911       120  [[[109, 141, 160], [105, 137, 156], [97, 129, ...    SingleFace\n",
       "912       120  [[[84, 94, 111], [81, 91, 108], [76, 86, 103],...    SingleFace\n",
       "913       209  [[[59, 64, 65], [57, 61, 62], [53, 57, 58], [5...    SingleFace\n",
       "\n",
       "[914 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FaceOnly Validation data is being read from md5 file\n",
    "validationDf = pd.read_pickle(\"../../../Data/ResizedData/FaceOnly/Validation.pkl\")\n",
    "validationDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validationX is being extracted from validationDf as wanted shape\n",
    "#validationX contains images with pixel values of data type np.uint8 in the range [0, 255]\n",
    "\n",
    "#Many pre-trained models, including the ones to be used within the scope of this project,\n",
    "#have been trained with images containing pixel values in the [-1, 1] range\n",
    "#In this way, the data will be symmetrical and the performance of the Backpropagation algorithm will be increased\n",
    "#See https://en.wikipedia.org/wiki/Backpropagation\n",
    "#See also https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data\n",
    "#Therefore, training will be performed by converting pixel values to this range with the simplest method (pixel / 127.5 - 1)\n",
    "\n",
    "#Converting pixel values to range [-1, 1] in this section is an option\n",
    "#Doing this once over the entire array now will be save time\n",
    "#This is not how the conversion will be done because of some memory problems in this project\n",
    "#Images are of data type np.uint8 when they are in the range [0, 255]\n",
    "#np.uint8 requires 1 byte memory while np.float32 requires 4 byte and np.float64 requires 8 byte\n",
    "#See https://www.educba.com/numpy-data-types/\n",
    "#When np.uint8 data type, images use about 1GB memory\n",
    "#Even if these pixel values are converted to np.float32 data type, it will need about 4GB of memory\n",
    "#The computer used for this project has 8GB Ram\n",
    "#Considering operating system requirements, memory required by the model, etc. 8GB Ram is not enough for this process\n",
    "#For this reason, this method is not preferred, although it will save time\n",
    "\n",
    "validationX = np.array(validationDf.ImageBGR.values.tolist())\n",
    "validationX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validationY is being extracted from validationDf as wanted shape\n",
    "validationY = np.array(validationDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "validationY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VGG16 pre-trained model is being loaded\n",
    "#The original VGG16 model was trained with images with size of (224, 224, 3) \n",
    "#in BGR color order and pixel values of [-1, 1] (zero centered) as default\n",
    "#See https://keras.io/api/applications/vgg/ for more information\n",
    "#Since images of dataset saved as size of (224, 224, 3) in BGR color order and pixel values of [0, 255]\n",
    "#dataset will be used by just converting the pixel values to the range [-1, 1]\n",
    "\n",
    "#Training will be performed in the sections between the last CNN block and the Output layer\n",
    "\n",
    "model = tf.keras.applications.vgg16.VGG16(include_top = False, input_shape = ((224, 224, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of layers of pre-trained model is being calculated except last CNN block\n",
    "nonTrainablePart = len(model.layers) - 4\n",
    "nonTrainablePart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Pre-trained model's layers except the last CNN block is being set to non-trainable\n",
    "for layer in model.layers[:nonTrainablePart]:\n",
    "    layer.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 423)               866727    \n",
      "=================================================================\n",
      "Total params: 71,160,039\n",
      "Trainable params: 63,524,775\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#The pre-trained model is being connected to the fully connected layer\n",
    "#A dropout layer is being added to the the model to prevent overfitting,\n",
    "#and the model is being completed with the addition of the output layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(2048, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2048, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(personDf.shape[0], activation = tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model is being compiled with Adam optimizer\n",
    "#Adam optimizer is a common used optimizer\n",
    "#See https://keras.io/api/optimizers/adam/\n",
    "#See also https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e\n",
    "#SparseCategoricalCrossentropy loss function is being used because of the label format of the data\n",
    "#SparseCategoricalAccuracy is being used as metric because of the label format of the data\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A class inherited from keras utils Sequence is being created\n",
    "class FitSequence(tf.keras.utils.Sequence):\n",
    "    \n",
    "    #Constructor method is being defined\n",
    "    def __init__(self, image, label, batchSize):\n",
    "        self.image, self.label = image, label\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        #A numpy array for image indexes is being created\n",
    "        #This array will be used to shuffle the data\n",
    "        self.index = np.arange(self.image.shape[0])\n",
    "    \n",
    "    #__len__ method is being defined\n",
    "    #This method will be used by the model to show the amount of progress of each epoch\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.image.shape[0] / float(self.batchSize)))\n",
    "    \n",
    "    #__getitem__ method is being defined\n",
    "    #The model will retrieve the batches it will use during training by calling this method\n",
    "    #With this method, the data to be used by the model can be manipulated\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #When the model requests data, the next batch size will be selected based on index array\n",
    "        indexPart = self.index[idx * self.batchSize : (idx + 1) * self.batchSize]\n",
    "        \n",
    "        #Before being sent to the model on demand pixel values will be converted to range [-1, 1]\n",
    "        #Doing this operation here means that it will be repeated as many epochs for each image and this wastes time\n",
    "        #This is how the conversion is being done because of some memory problem in this project\n",
    "        batchX = (self.image[indexPart] / 127.5) - 1\n",
    "        batchY = self.label[indexPart]\n",
    "        return np.array(batchX), np.array(batchY)\n",
    "    \n",
    "    #on_epoch_end method is being defined\n",
    "    #The model will call this method after each epoch is ended\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        #At the end of the epoch, the index array is being shuffled \n",
    "        #so that the data in the next epoch is returned in different orders\n",
    "        np.random.shuffle(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "260/260 [==============================] - 238s 839ms/step - loss: 5.7938 - accuracy: 0.0811 - val_loss: 5.3187 - val_accuracy: 0.0952\n",
      "Epoch 2/35\n",
      "260/260 [==============================] - 215s 827ms/step - loss: 5.5222 - accuracy: 0.0852 - val_loss: 5.3182 - val_accuracy: 0.0952\n",
      "Epoch 3/35\n",
      "260/260 [==============================] - 221s 848ms/step - loss: 5.5117 - accuracy: 0.0852 - val_loss: 5.3204 - val_accuracy: 0.0952\n",
      "Epoch 4/35\n",
      "260/260 [==============================] - 233s 894ms/step - loss: 5.5046 - accuracy: 0.0852 - val_loss: 5.3331 - val_accuracy: 0.0952\n",
      "Epoch 5/35\n",
      "260/260 [==============================] - 227s 873ms/step - loss: 5.5048 - accuracy: 0.0852 - val_loss: 5.3143 - val_accuracy: 0.0952\n",
      "Epoch 6/35\n",
      "260/260 [==============================] - 228s 875ms/step - loss: 5.5028 - accuracy: 0.0852 - val_loss: 5.3178 - val_accuracy: 0.0952\n",
      "Epoch 7/35\n",
      "260/260 [==============================] - 231s 889ms/step - loss: 5.4977 - accuracy: 0.0852 - val_loss: 5.3071 - val_accuracy: 0.0952\n",
      "Epoch 8/35\n",
      "260/260 [==============================] - 245s 941ms/step - loss: 5.4971 - accuracy: 0.0852 - val_loss: 5.3023 - val_accuracy: 0.0952\n",
      "Epoch 9/35\n",
      "260/260 [==============================] - 233s 894ms/step - loss: 5.4927 - accuracy: 0.0852 - val_loss: 5.3205 - val_accuracy: 0.0952\n",
      "Epoch 10/35\n",
      "260/260 [==============================] - 232s 891ms/step - loss: 5.4945 - accuracy: 0.0852 - val_loss: 5.3126 - val_accuracy: 0.0952\n",
      "Epoch 11/35\n",
      "260/260 [==============================] - 224s 863ms/step - loss: 5.4936 - accuracy: 0.0852 - val_loss: 5.3147 - val_accuracy: 0.0952\n",
      "Epoch 12/35\n",
      "260/260 [==============================] - 262s 1s/step - loss: 5.4917 - accuracy: 0.0852 - val_loss: 5.3075 - val_accuracy: 0.0952\n",
      "Epoch 13/35\n",
      "260/260 [==============================] - 225s 866ms/step - loss: 5.4890 - accuracy: 0.0852 - val_loss: 5.3148 - val_accuracy: 0.0952\n",
      "Epoch 14/35\n",
      "260/260 [==============================] - 223s 858ms/step - loss: 5.4909 - accuracy: 0.0852 - val_loss: 5.3296 - val_accuracy: 0.0952\n",
      "Epoch 15/35\n",
      "260/260 [==============================] - 239s 918ms/step - loss: 5.4922 - accuracy: 0.0852 - val_loss: 5.3032 - val_accuracy: 0.0952\n",
      "Epoch 16/35\n",
      "260/260 [==============================] - 238s 916ms/step - loss: 5.4851 - accuracy: 0.0852 - val_loss: 5.3046 - val_accuracy: 0.0952\n",
      "Epoch 17/35\n",
      "260/260 [==============================] - 245s 943ms/step - loss: 5.4887 - accuracy: 0.0852 - val_loss: 5.3118 - val_accuracy: 0.0952\n",
      "Epoch 18/35\n",
      "260/260 [==============================] - 231s 889ms/step - loss: 5.4880 - accuracy: 0.0852 - val_loss: 5.3178 - val_accuracy: 0.0952\n",
      "Epoch 19/35\n",
      "260/260 [==============================] - 235s 904ms/step - loss: 5.4862 - accuracy: 0.0852 - val_loss: 5.3168 - val_accuracy: 0.0952\n",
      "Epoch 20/35\n",
      "260/260 [==============================] - 276s 1s/step - loss: 5.4850 - accuracy: 0.0852 - val_loss: 5.3216 - val_accuracy: 0.0952\n",
      "Epoch 21/35\n",
      "260/260 [==============================] - 226s 868ms/step - loss: 5.4851 - accuracy: 0.0852 - val_loss: 5.3079 - val_accuracy: 0.0952\n",
      "Epoch 22/35\n",
      "260/260 [==============================] - 250s 963ms/step - loss: 5.4843 - accuracy: 0.0852 - val_loss: 5.3145 - val_accuracy: 0.0952\n",
      "Epoch 23/35\n",
      "260/260 [==============================] - 251s 965ms/step - loss: 5.4825 - accuracy: 0.0852 - val_loss: 5.3087 - val_accuracy: 0.0952\n",
      "Epoch 24/35\n",
      "260/260 [==============================] - 251s 965ms/step - loss: 5.4846 - accuracy: 0.0852 - val_loss: 5.3159 - val_accuracy: 0.0952\n",
      "Epoch 25/35\n",
      "260/260 [==============================] - 250s 963ms/step - loss: 5.4814 - accuracy: 0.0852 - val_loss: 5.3114 - val_accuracy: 0.0952\n",
      "Epoch 26/35\n",
      "260/260 [==============================] - 248s 952ms/step - loss: 5.4832 - accuracy: 0.0852 - val_loss: 5.3127 - val_accuracy: 0.0952\n",
      "Epoch 27/35\n",
      "260/260 [==============================] - 234s 900ms/step - loss: 5.4812 - accuracy: 0.0852 - val_loss: 5.3148 - val_accuracy: 0.0952\n",
      "Epoch 28/35\n",
      "260/260 [==============================] - 252s 967ms/step - loss: 5.4840 - accuracy: 0.0852 - val_loss: 5.3043 - val_accuracy: 0.0952\n",
      "Epoch 29/35\n",
      "260/260 [==============================] - 240s 923ms/step - loss: 5.4822 - accuracy: 0.0852 - val_loss: 5.3112 - val_accuracy: 0.0952\n",
      "Epoch 30/35\n",
      "260/260 [==============================] - 240s 924ms/step - loss: 5.4820 - accuracy: 0.0852 - val_loss: 5.3110 - val_accuracy: 0.0952\n",
      "Epoch 31/35\n",
      "260/260 [==============================] - 284s 1s/step - loss: 5.4814 - accuracy: 0.0852 - val_loss: 5.3216 - val_accuracy: 0.0952\n",
      "Epoch 32/35\n",
      "260/260 [==============================] - 247s 948ms/step - loss: 5.4815 - accuracy: 0.0852 - val_loss: 5.3160 - val_accuracy: 0.0952\n",
      "Epoch 33/35\n",
      "260/260 [==============================] - 246s 946ms/step - loss: 5.4792 - accuracy: 0.0852 - val_loss: 5.3045 - val_accuracy: 0.0952\n",
      "Epoch 34/35\n",
      "260/260 [==============================] - 246s 945ms/step - loss: 5.4825 - accuracy: 0.0852 - val_loss: 5.3153 - val_accuracy: 0.0952\n",
      "Epoch 35/35\n",
      "260/260 [==============================] - 246s 946ms/step - loss: 5.4791 - accuracy: 0.0852 - val_loss: 5.3103 - val_accuracy: 0.0952\n"
     ]
    }
   ],
   "source": [
    "#model is being trained with 35 epochs and 16 batchSize using GPU\n",
    "#A small batchSize value is being chosen to prevent GPU memory problem\n",
    "#Large batchSize reduce training time while also generally providing better results\n",
    "with tf.device(myGPU):\n",
    "    trainingHistory = model.fit(\n",
    "        FitSequence(trainingX, trainingY, 16),\n",
    "        epochs = 35,\n",
    "        validation_data = FitSequence(validationX, validationY, 16)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Chart is being drawn\n",
    "\n",
    "#Chart Values\n",
    "plt.plot(trainingHistory.history['loss'])\n",
    "plt.plot(trainingHistory.history['val_loss'])\n",
    "\n",
    "#Chart Tittle\n",
    "plt.title('Model Loss Chart')\n",
    "\n",
    "#Chart Labels\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#Chart Lines\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "#Show Method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Chart is being drawn\n",
    "\n",
    "#Chart Values\n",
    "plt.plot(trainingHistory.history['accuracy'])\n",
    "plt.plot(trainingHistory.history['val_accuracy'])\n",
    "\n",
    "#Chart Tittle\n",
    "plt.title('model accuracy')\n",
    "\n",
    "#Chart Labels\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "#Chart Lines\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "#Show Method\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>ImageBGR</th>\n",
       "      <th>DetectionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270</td>\n",
       "      <td>[[[0, 2, 13], [0, 1, 11], [0, 0, 8], [0, 0, 7]...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>[[[9, 11, 22], [9, 11, 23], [10, 12, 25], [11,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>[[[43, 69, 53], [32, 58, 42], [13, 37, 22], [6...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>[[[161, 124, 104], [160, 124, 105], [159, 125,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401</td>\n",
       "      <td>[[[8, 15, 142], [7, 15, 142], [6, 15, 142], [6...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>291</td>\n",
       "      <td>[[[157, 175, 176], [152, 170, 173], [143, 161,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>302</td>\n",
       "      <td>[[[76, 82, 89], [76, 82, 89], [77, 82, 90], [7...</td>\n",
       "      <td>MultipleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>401</td>\n",
       "      <td>[[[39, 42, 46], [38, 41, 45], [37, 40, 44], [3...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>340</td>\n",
       "      <td>[[[11, 16, 25], [11, 16, 24], [12, 15, 23], [1...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>293</td>\n",
       "      <td>[[[154, 178, 202], [155, 179, 203], [157, 181,...</td>\n",
       "      <td>SingleFace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PersonID                                           ImageBGR DetectionType\n",
       "0         270  [[[0, 2, 13], [0, 1, 11], [0, 0, 8], [0, 0, 7]...    SingleFace\n",
       "1          80  [[[9, 11, 22], [9, 11, 23], [10, 12, 25], [11,...    SingleFace\n",
       "2          14  [[[43, 69, 53], [32, 58, 42], [13, 37, 22], [6...    SingleFace\n",
       "3         120  [[[161, 124, 104], [160, 124, 105], [159, 125,...    SingleFace\n",
       "4         401  [[[8, 15, 142], [7, 15, 142], [6, 15, 142], [6...    SingleFace\n",
       "..        ...                                                ...           ...\n",
       "910       291  [[[157, 175, 176], [152, 170, 173], [143, 161,...    SingleFace\n",
       "911       302  [[[76, 82, 89], [76, 82, 89], [77, 82, 90], [7...  MultipleFace\n",
       "912       401  [[[39, 42, 46], [38, 41, 45], [37, 40, 44], [3...    SingleFace\n",
       "913       340  [[[11, 16, 25], [11, 16, 24], [12, 15, 23], [1...    SingleFace\n",
       "914       293  [[[154, 178, 202], [155, 179, 203], [157, 181,...    SingleFace\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FaceOnly Test data is being read from md5 file\n",
    "testDf = pd.read_pickle(\"../../../Data/ResizedData/FaceOnly/Test.pkl\")\n",
    "testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testX is being extracted from testDf as wanted shape\n",
    "testX = np.array(testDf.ImageBGR.values.tolist())\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testY is being extracted from testDf as wanted shape\n",
    "testY = np.array(testDf.PersonID.values.tolist()).reshape((-1,1))\n",
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 41s 701ms/step - loss: 5.3426 - accuracy: 0.0973\n"
     ]
    }
   ],
   "source": [
    "#Model is being evaluated with test data\n",
    "#Sequence class is being also used for evaluation to convert test data into the same format as training data\n",
    "testResult = model.evaluate(FitSequence(testX, testY, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.342550277709961\n"
     ]
    }
   ],
   "source": [
    "#Test Loss is being Printed\n",
    "print('Test Loss: ' + str(testResult[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0972677618265152\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy is being Printed\n",
    "print('Test Accuracy: ' + str(testResult[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training9 Inference\n",
    "\n",
    "By looking at the charts, it can be seen that learning does not take place.\n",
    "\n",
    "The model has not learned enough to have any success even on the Training data, even overfitting did not occur.\n",
    "\n",
    "It can even be seen that the accuracy values are stuck and do not change throughout the training.\n",
    "\n",
    "Performance can be improved by trying Hyperparameter Optimization methods.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Hyperparameter_optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39AI",
   "language": "python",
   "name": "py39ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
